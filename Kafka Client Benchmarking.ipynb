{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Kafka Client Benchmarking\n",
    "\n",
    "[Kafka](http://kafka.apache.org/) is an incredibly powerful service that can help you process huge streams of data. It is written in Scala and has been undergoing lots of changes. Historically, the JVM clients have been better supported then those in the Python ecosystem. However, this doesn't need to be case! The Kafka binary protocol has largely solidified and many people in open source community are working to provide first class support for non-JVM languages.\n",
    "\n",
    "In this post we will benchmark the three main Python Kafka clients. [pykafka](https://github.com/Parsely/pykafka), [python-kafka](https://github.com/dpkp/kafka-python) and the newest arrival [confluent-kafka-client](https://github.com/confluentinc/confluent-kafka-python). This will also be useful as an introduction to the different clients in the python ecosystem and their high level APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Setup and Caveat\n",
    "We are going to keep the setup simple and run a single broker that will auto-create topics with one partition. Please note this is a contrived setup. Any production deployment would be multi-broker and likely more partitions, but for simplicity sake we are going to use one.\n",
    "\n",
    "### The Plan\n",
    "- Spin up a single Kafka 0.9 broker\n",
    "\n",
    "- We are going to produce and consume 1 million 100 bytes messages with each client.\n",
    "\n",
    "- We will require a producer `acks` of 1. meaning that only the leader (we one have one broker anyways) needs to ack a message. Increasing this will ensure your data is not lost due to broker failure but will slow down production.\n",
    "\n",
    "I ran these tests within Vagrant hosted on a MacBook Pro 2.2Ghz i7. \n",
    "\n",
    "### Caveat\n",
    "Like all benchmarks, take this with a grain of salt. A single broker on a local machine is hardly a production deployment. All settings are largely left to their defaults. Also, the amount of file caching broker does really help the client consumption speed. To help combat this we will rerun each consumption test to compensate for caching recently accessed data. \n",
    "\n",
    "Even with all the normal stipulations, I hope you find this informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing clients can be complicated by the fact of some C extensions. We are big fans of [conda](http://conda.pydata.org/docs/) and maintain python3 linux builds of some of these clients [here](https://anaconda.org/ActivisionGameScience) with recipes [here](https://github.com/ActivisionGameScience/ags_conda_recipes).\n",
    "\n",
    "The easiest way is to use conda to install the clients:\n",
    "\n",
    "     conda create -n kafka-benchmark python=3 ipython jupyter pandas seaborn -y\n",
    "     source activate kafka-benchmark\n",
    "     conda install -c activisiongamescience confluent-kafka pykafka -y # will also get librdkafka\n",
    "     pip install kafka-python aiokafka nest_asyncio # pure python version is easy to install\n",
    "\n",
    "     \n",
    "If you would like to run this notebook please find the full repo [here](https://github.com/ActivisionGameScience/python-kafka-benchmark)\n",
    "     \n",
    "Included in this repo is a [docker-compose](https://github.com/ActivisionGameScience/python-kafka-benchmark/blob/master/docker-compose.yml) file that will spin up a single kafka 0.9 broker and zookeeper instance locally. We can shell out and start it with docker compose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating network \"pythonkafkabenchmark_default\" with the default driver\n",
      "Creating pythonkafkabenchmark_zookeeper_1\n",
      "Creating pythonkafkabenchmark_kafka_1\n"
     ]
    }
   ],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'kafkatestkafkatestkafkatestkafkatestkafkatestkafkatestkafkatestkafkatestkafkatestkafkatestkafkatestk'\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "msg_count = 1000000\n",
    "msg_size = 100\n",
    "msg_payload = ('kafkatest' * 20).encode()[:msg_size]\n",
    "print(msg_payload)\n",
    "print(len(msg_payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bootstrap_servers = 'localhost:9092' # change if your brokers live else where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "producer_timings = {}\n",
    "consumer_timings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_thoughput(timing, n_messages=1000000, msg_size=100):\n",
    "    print(\"Processed {0} messsages in {1:.2f} seconds\".format(n_messages, timing))\n",
    "    print(\"{0:.2f} MB/s\".format((msg_size * n_messages) / timing / (1024*1024)))\n",
    "    print(\"{0:.2f} Msgs/s\".format(n_messages / timing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [pykafka](https://github.com/Parsely/pykafka)\n",
    "\n",
    "The first client we will examine is pykafka. It the client we have to most experience with and have used with success. If tries less hard to replicate the existing java client API. It also has a partition balancing code for kafka broker version 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pykafka import KafkaClient\n",
    "\n",
    "def pykafka_producer_performance(use_rdkafka=False):\n",
    "    \n",
    "    # Setup client\n",
    "    client = KafkaClient(hosts=bootstrap_servers)\n",
    "    topic = client.topics[b'pykafka-test-topic']\n",
    "    producer = topic.get_producer(use_rdkafka=use_rdkafka)\n",
    "\n",
    "    msgs_produced = 0\n",
    "    produce_start = time.time()\n",
    "    for i in range(msg_count):\n",
    "        # Start producing\n",
    "        producer.produce(msg_payload)\n",
    "                     \n",
    "    producer.stop() # Will flush background queue\n",
    " \n",
    "    return time.time() - produce_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 messsages in 57.32 seconds\n",
      "1.66 MB/s\n",
      "17446.37 Msgs/s\n"
     ]
    }
   ],
   "source": [
    "producer_timings['pykafka_producer'] = pykafka_producer_performance()\n",
    "calculate_thoughput(producer_timings['pykafka_producer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are monitoring this function, you will notice that the produce loop completes and then the function stalls at the `produce.stop()`. This is because the producer is asynchronous and batches produce calls to Kafka. Kafka's speed comes from the ability to batch many message together. To take advantage of this, the client will keep a buffer of messages in the background and batch them. So, when you call `producer.produce` you are performing no external I/O. That message is queued in an in-memory buffer and the method returns immediately. So we are able to load the in-memory buffer faster then pykafka can send them to kafka. `producer.stop()` will block until all messages are sent.\n",
    "\n",
    "So when producing messages make sure you allow the producer to flush the remaining messages before you exit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to ensure that the messages where produced is to check the topic offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: OffsetPartitionResponse(offset=[0], err=0)}\n",
      "{0: OffsetPartitionResponse(offset=[1000000], err=0)}\n"
     ]
    }
   ],
   "source": [
    "client = KafkaClient(hosts=bootstrap_servers)\n",
    "topic = client.topics[b'pykafka-test-topic']\n",
    "print(topic.earliest_available_offsets())\n",
    "print(topic.latest_available_offsets())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pykafka has an optional producer backend that wraps the [librdkafka](https://github.com/edenhill/librdkafka) package. librdkafka is a pure C kafka client and holds very impressive benchmarks. Let rerun our pykafka producer test with rdkafka enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 messsages in 15.72 seconds\n",
      "6.06 MB/s\n",
      "63595.38 Msgs/s\n"
     ]
    }
   ],
   "source": [
    "producer_timings['pykafka_producer_rdkafka'] = pykafka_producer_performance(use_rdkafka=True)\n",
    "calculate_thoughput(producer_timings['pykafka_producer_rdkafka'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pykafka_consumer_performance(use_rdkafka=False):\n",
    "    # Setup client\n",
    "    client = KafkaClient(hosts=bootstrap_servers)\n",
    "    topic = client.topics[b'pykafka-test-topic']\n",
    "\n",
    "    msg_consumed_count = 0\n",
    "    \n",
    "    consumer_start = time.time()\n",
    "    # Consumer starts polling messages in background thread, need to start timer here\n",
    "    consumer = topic.get_simple_consumer(use_rdkafka=use_rdkafka)\n",
    "\n",
    "    while True:\n",
    "        msg = consumer.consume()\n",
    "        if msg:\n",
    "            msg_consumed_count += 1\n",
    "\n",
    "        if msg_consumed_count >= msg_count:\n",
    "            break\n",
    "                        \n",
    "    consumer_timing = time.time() - consumer_start\n",
    "    consumer.stop()    \n",
    "    return consumer_timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 messsages in 29.43 seconds\n",
      "3.24 MB/s\n",
      "33976.94 Msgs/s\n"
     ]
    }
   ],
   "source": [
    "_ = pykafka_consumer_performance(use_rdkafka=False)\n",
    "consumer_timings['pykafka_consumer'] = pykafka_consumer_performance(use_rdkafka=False)\n",
    "calculate_thoughput(consumer_timings['pykafka_consumer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 messsages in 6.09 seconds\n",
      "15.67 MB/s\n",
      "164311.50 Msgs/s\n"
     ]
    }
   ],
   "source": [
    "# run it once thorough to warm the cache\n",
    "_ = pykafka_consumer_performance(use_rdkafka=True)\n",
    "consumer_timings['pykafka_consumer_rdkafka'] = pykafka_consumer_performance(use_rdkafka=True)\n",
    "calculate_thoughput(consumer_timings['pykafka_consumer_rdkafka'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [kafka-python](https://github.com/dpkp/kafka-python)\n",
    "kafka-python aims to replicate the java client api exactly. This is a key difference with pykafka, which trys to maintains \"pythonic\" api. In earlier versions of kafka, partition balancing was left to the client. Pykafka was the only python client to implement this feature. However, with kafka 0.9 the broker provides this, so the lack of support within kafka-python is less important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "def python_kafka_producer_performance():\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "    producer_start = time.time()\n",
    "    topic = 'python-kafka-topic'\n",
    "    for i in range(msg_count):\n",
    "        producer.send(topic, msg_payload)\n",
    "        \n",
    "    producer.flush() # clear all local buffers and produce pending messages\n",
    "        \n",
    "    return time.time() - producer_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 messsages in 46.12 seconds\n",
      "2.07 MB/s\n",
      "21681.17 Msgs/s\n"
     ]
    }
   ],
   "source": [
    "producer_timings['python_kafka_producer'] = python_kafka_producer_performance()\n",
    "calculate_thoughput(producer_timings['python_kafka_producer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "def python_kafka_consumer_performance():\n",
    "    topic = 'python-kafka-topic'\n",
    "\n",
    "    consumer = KafkaConsumer(\n",
    "        bootstrap_servers=bootstrap_servers,\n",
    "        auto_offset_reset = 'earliest', # start at earliest topic\n",
    "        group_id = None # do no offest commit\n",
    "    )\n",
    "    msg_consumed_count = 0\n",
    "            \n",
    "    consumer_start = time.time()\n",
    "    consumer.subscribe([topic])\n",
    "    for msg in consumer:\n",
    "        msg_consumed_count += 1\n",
    "        \n",
    "        if msg_consumed_count >= msg_count:\n",
    "            break\n",
    "                    \n",
    "    consumer_timing = time.time() - consumer_start\n",
    "    consumer.close()    \n",
    "    return consumer_timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 messsages in 13.96 seconds\n",
      "6.83 MB/s\n",
      "71616.22 Msgs/s\n"
     ]
    }
   ],
   "source": [
    "_ = python_kafka_consumer_performance()\n",
    "consumer_timings['python_kafka_consumer'] = python_kafka_consumer_performance()\n",
    "calculate_thoughput(consumer_timings['python_kafka_consumer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [confluent-kafka-python](https://github.com/confluentinc/confluent-kafka-python)\n",
    "\n",
    "With the latest release of the Confluent platform, there is a new python client on the scene. confluent-kafka-python is a python wrapper around librdkafka and is largely built by the same author. The underlying library is basis for most non-JVM clients out there. We have already mentioned it earlier when looking at pykafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import confluent_kafka\n",
    "topic = 'confluent-kafka-topic'\n",
    "\n",
    "def confluent_kafka_producer_performance():\n",
    "    \n",
    "    topic = 'confluent-kafka-topic'\n",
    "    conf = {'bootstrap.servers': bootstrap_servers}\n",
    "    producer = confluent_kafka.Producer(**conf)\n",
    "    messages_to_retry = 0\n",
    "\n",
    "    producer_start = time.time()\n",
    "    for i in range(msg_count):\n",
    "        # try:\n",
    "        producer.poll(0)\n",
    "        producer.produce(topic, value=msg_payload)\n",
    "        producer.poll(0)\n",
    "        if i % 10000 == 0:\n",
    "            producer.flush()\n",
    "        # except BufferError as e:\n",
    "        #     messages_to_retry += 1\n",
    "    #\n",
    "    # # hacky retry messages that over filled the local buffer\n",
    "    # for i in range(messages_to_retry):\n",
    "    #     producer.poll(0)\n",
    "    #     try:\n",
    "    #         producer.produce(topic, value=msg_payload)\n",
    "    #     except BufferError as e:\n",
    "    #         producer.poll(0)\n",
    "    #         producer.produce(topic, value=msg_payload)\n",
    "\n",
    "    producer.flush()\n",
    "            \n",
    "    return time.time() - producer_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 messsages in 3.14 seconds\n",
      "30.34 MB/s\n",
      "318085.98 Msgs/s\n"
     ]
    }
   ],
   "source": [
    "producer_timings['confluent_kafka_producer'] = confluent_kafka_producer_performance()\n",
    "calculate_thoughput(producer_timings['confluent_kafka_producer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = KafkaClient(hosts=bootstrap_servers)\n",
    "topic = client.topics[b'confluent-kafka-topic']\n",
    "print(topic.earliest_available_offsets())\n",
    "print(topic.latest_available_offsets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import confluent_kafka\n",
    "import uuid\n",
    "\n",
    "def confluent_kafka_consumer_performance():\n",
    "    \n",
    "    topic = 'confluent-kafka-topic'\n",
    "    msg_consumed_count = 0\n",
    "    conf = {'bootstrap.servers': bootstrap_servers,\n",
    "            'group.id': uuid.uuid1(),\n",
    "            'session.timeout.ms': 6000,\n",
    "            'default.topic.config': {\n",
    "                'auto.offset.reset': 'earliest'\n",
    "            }\n",
    "    }\n",
    "\n",
    "    consumer = confluent_kafka.Consumer(**conf)\n",
    "\n",
    "    consumer_start = time.time()\n",
    "    # This is the same as pykafka, subscribing to a topic will start a background thread\n",
    "    consumer.subscribe([topic])\n",
    "\n",
    "    while True:\n",
    "        msg = consumer.poll(1)\n",
    "        if msg:\n",
    "            msg_consumed_count += 1\n",
    "                         \n",
    "        if msg_consumed_count >= msg_count:\n",
    "            break\n",
    "                    \n",
    "    consumer_timing = time.time() - consumer_start\n",
    "    consumer.close()    \n",
    "    return consumer_timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 messsages in 3.83 seconds\n",
      "24.93 MB/s\n",
      "261407.91 Msgs/s\n"
     ]
    }
   ],
   "source": [
    "_ = confluent_kafka_consumer_performance() # Warm cache\n",
    "consumer_timings['confluent_kafka_consumer'] = confluent_kafka_consumer_performance()\n",
    "calculate_thoughput(consumer_timings['confluent_kafka_consumer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confluent_kafka client is crushingly fast. It can consume over 250K messages a second from a single broker. Note that the raw C client has been benchmarked at over 3 million messages/sec, so you see how much overhead python adds. But on the side of developer speed, you don't have to code in C!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import aiokafka\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "def aiokafka_producer_performance():\n",
    "    async def aiokafka_producer_performance():\n",
    "        producer = aiokafka.AIOKafkaProducer()\n",
    "        await producer.start()\n",
    "        topic = \"aiokafka-test-topic\"\n",
    "        produce_start = time.time()\n",
    "        for i in range(msg_count):\n",
    "            # Start producing\n",
    "            await producer.send(topic, msg_payload)\n",
    "\n",
    "        await producer.flush() # Will flush background queue\n",
    "        await producer.stop()\n",
    "        return time.time() - produce_start\n",
    "\n",
    "    return asyncio.get_event_loop().run_until_complete(aiokafka_producer_performance())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 messsages in 19.07 seconds\n",
      "5.00 MB/s\n",
      "52445.85 Msgs/s\n"
     ]
    }
   ],
   "source": [
    "producer_timings['aio_kafka_producer'] = aiokafka_producer_performance()\n",
    "calculate_thoughput(producer_timings['aio_kafka_producer'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-32-983e08dc9308>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mseaborn\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0msns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_line_magic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'pylab'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'inline'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "consumer_df = pd.DataFrame.from_dict(consumer_timings, orient='index').rename(columns={0: 'time_in_seconds'})\n",
    "producer_df = pd.DataFrame.from_dict(producer_timings, orient='index').rename(columns={0: 'time_in_seconds'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "consumer_df['MBs/s'] = (len(msg_payload) * msg_count) / consumer_df.time_in_seconds / (1024*1024)\n",
    "producer_df['MBs/s'] = (len(msg_payload) * msg_count) / producer_df.time_in_seconds / (1024*1024)\n",
    "\n",
    "consumer_df['Msgs/s'] = msg_count / consumer_df.time_in_seconds\n",
    "producer_df['Msgs/s'] = msg_count / producer_df.time_in_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                          time_in_seconds      MBs/s         Msgs/s\naio_kafka_producer              19.067286   5.001626   52445.848834\nconfluent_kafka_producer         3.143804  30.335043  318085.980129\npython_kafka_producer           46.122980   2.067677   21681.166253",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time_in_seconds</th>\n      <th>MBs/s</th>\n      <th>Msgs/s</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>aio_kafka_producer</th>\n      <td>19.067286</td>\n      <td>5.001626</td>\n      <td>52445.848834</td>\n    </tr>\n    <tr>\n      <th>confluent_kafka_producer</th>\n      <td>3.143804</td>\n      <td>30.335043</td>\n      <td>318085.980129</td>\n    </tr>\n    <tr>\n      <th>python_kafka_producer</th>\n      <td>46.122980</td>\n      <td>2.067677</td>\n      <td>21681.166253</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "producer_df.sort_index(inplace=True)\n",
    "producer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                       time_in_seconds     MBs/s        Msgs/s\npython_kafka_consumer        13.963318  6.829855  71616.215584",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time_in_seconds</th>\n      <th>MBs/s</th>\n      <th>Msgs/s</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>python_kafka_consumer</th>\n      <td>13.963318</td>\n      <td>6.829855</td>\n      <td>71616.215584</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consumer_df.sort_index(inplace=True)\n",
    "consumer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "matplotlib is required for plotting when the default backend \"matplotlib\" is selected.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-37-f92e79cf5417>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mproducer_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkind\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'bar'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msubplots\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfigsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtitle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"Producer Comparison\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venvs/venv/lib/python3.7/site-packages/pandas/plotting/_core.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    890\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    891\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 892\u001B[0;31m         \u001B[0mplot_backend\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_plot_backend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"backend\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    893\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    894\u001B[0m         x, y, kind, kwargs = self._get_call_args(\n",
      "\u001B[0;32m~/venvs/venv/lib/python3.7/site-packages/pandas/plotting/_core.py\u001B[0m in \u001B[0;36m_get_plot_backend\u001B[0;34m(backend)\u001B[0m\n\u001B[1;32m   1812\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_backends\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mbackend\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1813\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1814\u001B[0;31m     \u001B[0mmodule\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_load_backend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbackend\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1815\u001B[0m     \u001B[0m_backends\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mbackend\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1816\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venvs/venv/lib/python3.7/site-packages/pandas/plotting/_core.py\u001B[0m in \u001B[0;36m_load_backend\u001B[0;34m(backend)\u001B[0m\n\u001B[1;32m   1755\u001B[0m                 \u001B[0;34m\"matplotlib is required for plotting when the \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1756\u001B[0m                 \u001B[0;34m'default backend \"matplotlib\" is selected.'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1757\u001B[0;31m             ) from None\n\u001B[0m\u001B[1;32m   1758\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1759\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: matplotlib is required for plotting when the default backend \"matplotlib\" is selected."
     ]
    }
   ],
   "source": [
    "producer_df.plot(kind='bar', subplots=True, figsize=(10, 10), title=\"Producer Comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "consumer_df.plot(kind='bar', subplots=True, figsize=(10, 10), title=\"Consumer Comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations\n",
    "\n",
    "This is not a complete overview of these clients. There are many trade-offs to consider before you jump and pick one. First, is the confluent and rdkafka version of pykafka are C backed and will not work with pypy. Also, packaging and distributing python packages with C extension can be a pain. We are big fans of conda to help alleviate this pain. We have package both [pykafka]((https://anaconda.org/ActivisionGameScience/pykafka), [confluent-kafka-client](https://anaconda.org/ActivisionGameScience/confluent-kafka) and [librdkafka](https://anaconda.org/ActivisionGameScience/librdkafka) on our [public conda channel](https://anaconda.org/ActivisionGameScience).\n",
    "\n",
    "Another limitation of this benchmark approach that negatively affects the performance of pykafka and confluent_kafka is the lack of other work other then blindly ripping through messages. Pykafka uses a background thread to consume messages before you ever call `consume()`. The vagaries of python threading and limitations of the GIL are beyond the scope of this post but keep in mind that pykafka's background will continue to poll brokers while you do work in your program. This effect is more pronounced for any work that is I/O bound. \n",
    "\n",
    "The confluent_kafka client was released on May 25th, so while the underlying librdkafka is hardened as widely used, the python client is very fresh. Also, some of the metadata APIs are not exposed to the client. Specifically, we have found issues with the Offset API and Offset Commit/Fetch API, so getting the starting and ending offset for a partition and the current lag of a consumer group is not straight forward. Confluent seems committed to the client, so I'm sure a sane client metadata api will be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Not to devolve this post into the ravings of a petulant language zealot, but why should the JVM get all the goodies first! It is really exciting to see the python ecosystem around Kafka develop. Having first class Python support for Kafka is huge for our work here at Activision and for anyone who is interested in stream processing and streaming analytics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!docker-compose down # Cleanup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}